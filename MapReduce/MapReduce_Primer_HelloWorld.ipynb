{"cells":[{"cell_type":"markdown","metadata":{"id":"GzbmlR27wh6e"},"source":["<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n","\n","# MapReduce: A Primer with <code>Hello World!</code>\n","<br>\n","<br>\n","\n","For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n","\n","> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n","\n","(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n","\n","We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n","\n","> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n","\n","MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n","\n","Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"]},{"cell_type":"markdown","metadata":{"id":"uUbM5R0GwwYw"},"source":["# Download core Hadoop"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93171,"status":"ok","timestamp":1714207225307,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"jDgQtQlzw8bL","outputId":"5a8b5d66-926e-444d-cbe5-34db480e4f5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"]}],"source":["HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n","\n","import requests\n","import os\n","import tarfile\n","\n","def download_and_extract_targz(url):\n","    response = requests.get(url)\n","    filename = url.rsplit('/', 1)[-1]\n","    HADOOP_HOME = filename[:-7]\n","    # set HADOOP_HOME environment variable\n","    os.environ['HADOOP_HOME'] = HADOOP_HOME\n","    if os.path.isdir(HADOOP_HOME):\n","      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n","      return\n","    if response.status_code == 200:\n","        with open(filename, 'wb') as file:\n","            file.write(response.content)\n","        with tarfile.open(filename, 'r:gz') as tar_ref:\n","            extract_path = tar_ref.extractall(path='.')\n","            # Get the names of all members (files and directories) in the archive\n","            all_members = tar_ref.getnames()\n","            # If there is a top-level directory, get its name\n","            if all_members:\n","              top_level_directory = all_members[0]\n","              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n","    else:\n","        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n","\n","\n","download_and_extract_targz(HADOOP_URL)"]},{"cell_type":"markdown","metadata":{"id":"3yvb5cw9xEbh"},"source":["# Set environment variables"]},{"cell_type":"markdown","metadata":{"id":"u6lkrz1dxIiO"},"source":["## Set `HADOOP_HOME` and `PATH`"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1714207225308,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"s7maAwaFxBT_","outputId":"f0939bab-60c0-418c-ae1a-126ac3700ba5"},"outputs":[{"name":"stdout","output_type":"stream","text":["HADOOP_HOME is hadoop-3.4.0\n","PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"]}],"source":["# HADOOP_HOME was set earlier when downloading Hadoop distribution\n","print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n","\n","os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n","print(\"PATH is {}\".format(os.environ['PATH']))"]},{"cell_type":"markdown","metadata":{"id":"4kzJ8cNoxPyK"},"source":["## Set `JAVA_HOME`\n","\n","While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1714207225308,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"SauFHVPOxL-Y","outputId":"0e4fd610-86d7-4266-8424-e775d8996c48"},"outputs":[{"name":"stdout","output_type":"stream","text":["Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"]}],"source":["import shutil\n","\n","# set variable JAVA_HOME (install Java if necessary)\n","def is_java_installed():\n","    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","    return os.environ['JAVA_HOME']\n","\n","def install_java():\n","    # Uncomment and modify the desired version\n","    # java_version= 'openjdk-11-jre-headless'\n","    # java_version= 'default-jre'\n","    # java_version= 'openjdk-17-jre-headless'\n","    # java_version= 'openjdk-18-jre-headless'\n","    java_version= 'openjdk-19-jre-headless'\n","\n","    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n","    try:\n","        cmd = f\"apt install -y {java_version}\"\n","        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n","        stdout_result = subprocess_output.stdout\n","        # Process the results as needed\n","        print(\"Done installing Java {}\".format(java_version))\n","        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n","    except subprocess.CalledProcessError as e:\n","        # Handle the error if the command returns a non-zero exit code\n","        print(\"Command failed with return code {}\".format(e.returncode))\n","        print(\"stdout: {}\".format(e.stdout))\n","\n","# Install Java if not available\n","if is_java_installed():\n","    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n","else:\n","    print(\"Installing Java\")\n","    install_java()"]},{"cell_type":"markdown","metadata":{"id":"6HFPVX84xbNd"},"source":["# Run a MapReduce job with Hadoop streaming"]},{"cell_type":"markdown","metadata":{"id":"_yVa55X1xmOb"},"source":["## Create a file\n","\n","Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714207225308,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"9Jz7mJkcxYxw"},"outputs":[],"source":["!echo \"Hello, World!\">./hello.txt"]},{"cell_type":"markdown","metadata":{"id":"zSh_Kr5Bxvst"},"source":["## Launch the MapReduce \"Hello, World!\" application\n","\n","Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n","\n","Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n","\n","**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6627,"status":"ok","timestamp":1714207231932,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"nb5JryK9xpPA","outputId":"3366621c-e740-46ed-9b06-c68ea9d5743b"},"outputs":[{"name":"stderr","output_type":"stream","text":["rm: `my_output': No such file or directory\n","2024-04-27 08:40:28,862 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-27 08:40:29,201 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-27 08:40:29,201 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-27 08:40:29,240 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-27 08:40:29,632 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-27 08:40:29,654 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-27 08:40:29,982 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local956078343_0001\n","2024-04-27 08:40:29,982 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-27 08:40:30,188 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-27 08:40:30,189 INFO mapreduce.Job: Running job: job_local956078343_0001\n","2024-04-27 08:40:30,197 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-27 08:40:30,199 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-27 08:40:30,205 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 08:40:30,205 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 08:40:30,243 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-27 08:40:30,248 INFO mapred.LocalJobRunner: Starting task: attempt_local956078343_0001_m_000000_0\n","2024-04-27 08:40:30,279 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 08:40:30,281 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 08:40:30,305 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-27 08:40:30,313 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-04-27 08:40:30,333 INFO mapred.MapTask: numReduceTasks: 1\n","2024-04-27 08:40:30,378 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-04-27 08:40:30,378 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-04-27 08:40:30,378 INFO mapred.MapTask: soft limit at 83886080\n","2024-04-27 08:40:30,378 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-04-27 08:40:30,378 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-04-27 08:40:30,381 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-04-27 08:40:30,384 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n","2024-04-27 08:40:30,390 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-04-27 08:40:30,391 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-04-27 08:40:30,391 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-04-27 08:40:30,392 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-04-27 08:40:30,392 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-04-27 08:40:30,392 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-04-27 08:40:30,393 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-04-27 08:40:30,394 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-04-27 08:40:30,394 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-04-27 08:40:30,394 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-04-27 08:40:30,397 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-04-27 08:40:30,398 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-04-27 08:40:30,427 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-27 08:40:30,434 INFO streaming.PipeMapRed: Records R/W=1/1\n","2024-04-27 08:40:30,435 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-27 08:40:30,435 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-27 08:40:30,437 INFO mapred.LocalJobRunner: \n","2024-04-27 08:40:30,438 INFO mapred.MapTask: Starting flush of map output\n","2024-04-27 08:40:30,438 INFO mapred.MapTask: Spilling map output\n","2024-04-27 08:40:30,438 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n","2024-04-27 08:40:30,438 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n","2024-04-27 08:40:30,448 INFO mapred.MapTask: Finished spill 0\n","2024-04-27 08:40:30,460 INFO mapred.Task: Task:attempt_local956078343_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-27 08:40:30,464 INFO mapred.LocalJobRunner: Records R/W=1/1\n","2024-04-27 08:40:30,465 INFO mapred.Task: Task 'attempt_local956078343_0001_m_000000_0' done.\n","2024-04-27 08:40:30,472 INFO mapred.Task: Final Counters for attempt_local956078343_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=854181\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=15\n","\t\tMap output materialized bytes=23\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tSpilled Records=1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=316669952\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","2024-04-27 08:40:30,472 INFO mapred.LocalJobRunner: Finishing task: attempt_local956078343_0001_m_000000_0\n","2024-04-27 08:40:30,473 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-27 08:40:30,479 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-04-27 08:40:30,479 INFO mapred.LocalJobRunner: Starting task: attempt_local956078343_0001_r_000000_0\n","2024-04-27 08:40:30,490 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 08:40:30,490 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 08:40:30,490 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-27 08:40:30,495 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1e604c69\n","2024-04-27 08:40:30,496 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-27 08:40:30,520 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-04-27 08:40:30,528 INFO reduce.EventFetcher: attempt_local956078343_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-04-27 08:40:30,561 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local956078343_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n","2024-04-27 08:40:30,566 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local956078343_0001_m_000000_0\n","2024-04-27 08:40:30,573 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n","2024-04-27 08:40:30,580 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-04-27 08:40:30,581 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-27 08:40:30,581 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-04-27 08:40:30,592 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-27 08:40:30,592 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n","2024-04-27 08:40:30,594 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n","2024-04-27 08:40:30,595 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n","2024-04-27 08:40:30,595 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-04-27 08:40:30,595 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-27 08:40:30,596 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n","2024-04-27 08:40:30,597 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-27 08:40:30,612 INFO mapred.Task: Task:attempt_local956078343_0001_r_000000_0 is done. And is in the process of committing\n","2024-04-27 08:40:30,615 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-27 08:40:30,616 INFO mapred.Task: Task attempt_local956078343_0001_r_000000_0 is allowed to commit now\n","2024-04-27 08:40:30,622 INFO output.FileOutputCommitter: Saved output of task 'attempt_local956078343_0001_r_000000_0' to file:/content/my_output\n","2024-04-27 08:40:30,623 INFO mapred.LocalJobRunner: reduce > reduce\n","2024-04-27 08:40:30,623 INFO mapred.Task: Task 'attempt_local956078343_0001_r_000000_0' done.\n","2024-04-27 08:40:30,624 INFO mapred.Task: Final Counters for attempt_local956078343_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141992\n","\t\tFILE: Number of bytes written=854231\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=23\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=1\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=316669952\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-04-27 08:40:30,624 INFO mapred.LocalJobRunner: Finishing task: attempt_local956078343_0001_r_000000_0\n","2024-04-27 08:40:30,624 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-04-27 08:40:31,194 INFO mapreduce.Job: Job job_local956078343_0001 running in uber mode : false\n","2024-04-27 08:40:31,195 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-04-27 08:40:31,196 INFO mapreduce.Job: Job job_local956078343_0001 completed successfully\n","2024-04-27 08:40:31,204 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=283906\n","\t\tFILE: Number of bytes written=1708412\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=15\n","\t\tMap output materialized bytes=23\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=23\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=2\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=633339904\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-04-27 08:40:31,204 INFO streaming.StreamJob: Output directory: my_output\n"]}],"source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -input hello.txt \\\n","    -output my_output \\\n","    -mapper '/bin/cat'"]},{"cell_type":"markdown","metadata":{"id":"OB_fX9u5x55y"},"source":["## Verify the result\n","\n","If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n","\n","Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1290,"status":"ok","timestamp":1714207233210,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"bnvEvYDfx2g4","outputId":"048bce7f-7324-44d0-f45b-eb9dc24d4c4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Check if MapReduce job was successful\n","_SUCCESS exists!\n"]}],"source":["%%bash\n","\n","echo \"Check if MapReduce job was successful\"\n","hdfs dfs -test -e my_output/_SUCCESS\n","if [ $? -eq 0 ]; then\n","\techo \"_SUCCESS exists!\"\n","fi"]},{"cell_type":"markdown","metadata":{"id":"BLMnBh44x_YR"},"source":["**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1595,"status":"ok","timestamp":1714207234802,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"ufAfmGUvx8jW","outputId":"72236139-708d-436f-955b-939a410b7329"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2 items\n","-rw-r--r--   1 root root          0 2024-04-27 08:40 my_output/_SUCCESS\n","-rw-r--r--   1 root root         15 2024-04-27 08:40 my_output/part-00000\n"]}],"source":["!hdfs dfs -ls my_output"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1714207234803,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"ZnKSahPzyCAn","outputId":"80c894b0-035a-4f52-aa86-9fddcf78d4b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 4\n","-rw-r--r-- 1 root root 15 Apr 27 08:40 part-00000\n","-rw-r--r-- 1 root root  0 Apr 27 08:40 _SUCCESS\n"]}],"source":["!ls -l my_output"]},{"cell_type":"markdown","metadata":{"id":"v9LmpcaMyG23"},"source":["The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":467,"status":"ok","timestamp":1714207241139,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"eL-Clat5yD8I","outputId":"17b88956-9ad3-4ce9-b022-eb48147f4c98"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello, World!\t\n"]}],"source":["!cat my_output/part-00000"]},{"cell_type":"markdown","metadata":{"id":"AmpHr_HyyMnM"},"source":["# MapReduce without specifying mapper or reducer\n","\n","In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n","\n","Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1135,"status":"ok","timestamp":1714207247126,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"ZPWL1AiXyJac","outputId":"56e0fad2-4a0f-48be-919e-50fea9ea5dfa"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-04-27 08:40:46,480 ERROR streaming.StreamJob: Unrecognized option: -h\n","Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n","Options:\n","  -input          <path> DFS input file(s) for the Map step.\n","  -output         <path> DFS output directory for the Reduce step.\n","  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n","  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n","  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n","  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n","                  Deprecated. Use generic option \"-files\" instead.\n","  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n","                  Optional. The input format class.\n","  -outputformat   <TextOutputFormat(default)|JavaClassName>\n","                  Optional. The output format class.\n","  -partitioner    <JavaClassName>  Optional. The partitioner class.\n","  -numReduceTasks <num> Optional. Number of reduce tasks.\n","  -inputreader    <spec> Optional. Input recordreader spec.\n","  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n","  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n","  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n","  -io             <identifier> Optional. Format to use for input to and output\n","                  from mapper/reducer commands\n","  -lazyOutput     Optional. Lazily create Output.\n","  -background     Optional. Submit the job and don't wait till it completes.\n","  -verbose        Optional. Print verbose output.\n","  -info           Optional. Print detailed usage.\n","  -help           Optional. Print help message.\n","\n","Generic options supported are:\n","-conf <configuration file>        specify an application configuration file\n","-D <property=value>               define a value for a given property\n","-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n","-jt <local|resourcemanager:port>  specify a ResourceManager\n","-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n","-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n","-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n","\n","The general command line syntax is:\n","command [genericOptions] [commandOptions]\n","\n","\n","For more details about these options:\n","Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n","\n","Try -help for more information\n","Streaming Command Failed!\n"]}],"source":["!mapred streaming -h"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4943,"status":"ok","timestamp":1714207282356,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"5H2MkIUPyQc2","outputId":"306239c5-8cbf-4bc9-a9e3-5bcf8c7681a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Deleted my_output\n"]},{"name":"stderr","output_type":"stream","text":["2024-04-27 08:41:18,500 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","2024-04-27 08:41:20,094 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-27 08:41:20,240 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-27 08:41:20,240 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-27 08:41:20,257 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-27 08:41:20,460 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-27 08:41:20,484 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-27 08:41:20,710 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local297148614_0001\n","2024-04-27 08:41:20,711 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-27 08:41:20,874 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-27 08:41:20,876 INFO mapreduce.Job: Running job: job_local297148614_0001\n","2024-04-27 08:41:20,882 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-27 08:41:20,885 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-27 08:41:20,891 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 08:41:20,894 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 08:41:20,935 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-27 08:41:20,941 INFO mapred.LocalJobRunner: Starting task: attempt_local297148614_0001_m_000000_0\n","2024-04-27 08:41:20,982 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 08:41:20,984 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 08:41:21,015 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-27 08:41:21,024 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-04-27 08:41:21,036 INFO mapred.MapTask: numReduceTasks: 1\n","2024-04-27 08:41:21,077 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-04-27 08:41:21,077 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-04-27 08:41:21,077 INFO mapred.MapTask: soft limit at 83886080\n","2024-04-27 08:41:21,077 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-04-27 08:41:21,077 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-04-27 08:41:21,082 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-04-27 08:41:21,090 INFO mapred.LocalJobRunner: \n","2024-04-27 08:41:21,090 INFO mapred.MapTask: Starting flush of map output\n","2024-04-27 08:41:21,090 INFO mapred.MapTask: Spilling map output\n","2024-04-27 08:41:21,090 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n","2024-04-27 08:41:21,090 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n","2024-04-27 08:41:21,099 INFO mapred.MapTask: Finished spill 0\n","2024-04-27 08:41:21,139 INFO mapred.Task: Task:attempt_local297148614_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-27 08:41:21,142 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n","2024-04-27 08:41:21,142 INFO mapred.Task: Task 'attempt_local297148614_0001_m_000000_0' done.\n","2024-04-27 08:41:21,149 INFO mapred.Task: Final Counters for attempt_local297148614_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=852085\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=22\n","\t\tMap output materialized bytes=30\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tSpilled Records=1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=22\n","\t\tTotal committed heap usage (bytes)=303038464\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","2024-04-27 08:41:21,149 INFO mapred.LocalJobRunner: Finishing task: attempt_local297148614_0001_m_000000_0\n","2024-04-27 08:41:21,150 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-27 08:41:21,153 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-04-27 08:41:21,153 INFO mapred.LocalJobRunner: Starting task: attempt_local297148614_0001_r_000000_0\n","2024-04-27 08:41:21,165 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 08:41:21,165 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 08:41:21,165 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-27 08:41:21,169 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3945f33f\n","2024-04-27 08:41:21,170 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-27 08:41:21,191 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-04-27 08:41:21,202 INFO reduce.EventFetcher: attempt_local297148614_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-04-27 08:41:21,234 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local297148614_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n","2024-04-27 08:41:21,239 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local297148614_0001_m_000000_0\n","2024-04-27 08:41:21,241 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n","2024-04-27 08:41:21,244 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-04-27 08:41:21,245 WARN io.ReadaheadPool: Failed readahead on ifile\n","EBADF: Bad file descriptor\n","\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n","\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:426)\n","\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:296)\n","\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:220)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","2024-04-27 08:41:21,245 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-27 08:41:21,247 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-04-27 08:41:21,252 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-27 08:41:21,252 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n","2024-04-27 08:41:21,254 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n","2024-04-27 08:41:21,254 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n","2024-04-27 08:41:21,255 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-04-27 08:41:21,257 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-27 08:41:21,259 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n","2024-04-27 08:41:21,260 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-27 08:41:21,266 INFO mapred.Task: Task:attempt_local297148614_0001_r_000000_0 is done. And is in the process of committing\n","2024-04-27 08:41:21,268 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-27 08:41:21,268 INFO mapred.Task: Task attempt_local297148614_0001_r_000000_0 is allowed to commit now\n","2024-04-27 08:41:21,269 INFO output.FileOutputCommitter: Saved output of task 'attempt_local297148614_0001_r_000000_0' to file:/content/my_output\n","2024-04-27 08:41:21,271 INFO mapred.LocalJobRunner: reduce > reduce\n","2024-04-27 08:41:21,271 INFO mapred.Task: Task 'attempt_local297148614_0001_r_000000_0' done.\n","2024-04-27 08:41:21,271 INFO mapred.Task: Final Counters for attempt_local297148614_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=142006\n","\t\tFILE: Number of bytes written=852143\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=30\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=1\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=303038464\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-04-27 08:41:21,271 INFO mapred.LocalJobRunner: Finishing task: attempt_local297148614_0001_r_000000_0\n","2024-04-27 08:41:21,272 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-04-27 08:41:21,880 INFO mapreduce.Job: Job job_local297148614_0001 running in uber mode : false\n","2024-04-27 08:41:21,881 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-04-27 08:41:21,882 INFO mapreduce.Job: Job job_local297148614_0001 completed successfully\n","2024-04-27 08:41:21,890 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=283920\n","\t\tFILE: Number of bytes written=1704228\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=22\n","\t\tMap output materialized bytes=30\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=30\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=2\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=22\n","\t\tTotal committed heap usage (bytes)=606076928\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-04-27 08:41:21,890 INFO streaming.StreamJob: Output directory: my_output\n"]}],"source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -input hello.txt \\\n","    -output my_output"]},{"cell_type":"markdown","metadata":{"id":"v7Ks3e96yXuB"},"source":["## Verify the result"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1658,"status":"ok","timestamp":1714207285587,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"cWAXvG0_yThc","outputId":"c93db86c-8ce8-4fe1-dbe6-c7b6b538aa8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Check if MapReduce job was successful\n","_SUCCESS exists!\n"]}],"source":["%%bash\n","\n","echo \"Check if MapReduce job was successful\"\n","hdfs dfs -test -e my_output/_SUCCESS\n","if [ $? -eq 0 ]; then\n","\techo \"_SUCCESS exists!\"\n","fi"]},{"cell_type":"markdown","metadata":{"id":"t40GgJ2Hya9P"},"source":["Show output"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1714207310977,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"I5APWEgoyaRS","outputId":"370d993e-41c6-48ee-8472-7b67f0a594ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\tHello, World!\n"]}],"source":["!cat my_output/part-00000"]},{"cell_type":"markdown","metadata":{"id":"mzfaMVKqyjpC"},"source":["What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."]},{"cell_type":"markdown","metadata":{"id":"lzIuWv7Myndc"},"source":["# Run a map-only MapReduce job\n","\n","Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n","\n","To run a MapReduce job _without_ reducer one needs to use the generic option\n","\n","    \\-D mapreduce.job.reduces=0\n","\n","(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5275,"status":"ok","timestamp":1714207327344,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"OdwKWyVRye27","outputId":"fe224bbc-6219-40b7-ecbd-f1794fde9f95"},"outputs":[{"name":"stdout","output_type":"stream","text":["Deleted my_output\n"]},{"name":"stderr","output_type":"stream","text":["2024-04-27 08:42:03,231 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","2024-04-27 08:42:04,854 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-27 08:42:04,983 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-27 08:42:04,983 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-27 08:42:04,999 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-27 08:42:05,215 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-27 08:42:05,232 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-27 08:42:05,451 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local401098912_0001\n","2024-04-27 08:42:05,451 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-27 08:42:05,626 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-27 08:42:05,627 INFO mapreduce.Job: Running job: job_local401098912_0001\n","2024-04-27 08:42:05,635 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-27 08:42:05,638 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-27 08:42:05,646 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 08:42:05,646 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 08:42:05,702 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-27 08:42:05,710 INFO mapred.LocalJobRunner: Starting task: attempt_local401098912_0001_m_000000_0\n","2024-04-27 08:42:05,738 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 08:42:05,739 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 08:42:05,771 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-27 08:42:05,783 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-04-27 08:42:05,797 INFO mapred.MapTask: numReduceTasks: 0\n","2024-04-27 08:42:05,833 INFO mapred.LocalJobRunner: \n","2024-04-27 08:42:05,843 INFO mapred.Task: Task:attempt_local401098912_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-27 08:42:05,844 INFO mapred.LocalJobRunner: \n","2024-04-27 08:42:05,845 INFO mapred.Task: Task attempt_local401098912_0001_m_000000_0 is allowed to commit now\n","2024-04-27 08:42:05,849 INFO output.FileOutputCommitter: Saved output of task 'attempt_local401098912_0001_m_000000_0' to file:/content/my_output\n","2024-04-27 08:42:05,850 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n","2024-04-27 08:42:05,850 INFO mapred.Task: Task 'attempt_local401098912_0001_m_000000_0' done.\n","2024-04-27 08:42:05,856 INFO mapred.Task: Final Counters for attempt_local401098912_0001_m_000000_0: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=852049\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=12\n","\t\tTotal committed heap usage (bytes)=337641472\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-04-27 08:42:05,857 INFO mapred.LocalJobRunner: Finishing task: attempt_local401098912_0001_m_000000_0\n","2024-04-27 08:42:05,859 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-27 08:42:06,632 INFO mapreduce.Job: Job job_local401098912_0001 running in uber mode : false\n","2024-04-27 08:42:06,635 INFO mapreduce.Job:  map 100% reduce 0%\n","2024-04-27 08:42:06,638 INFO mapreduce.Job: Job job_local401098912_0001 completed successfully\n","2024-04-27 08:42:06,646 INFO mapreduce.Job: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=852049\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=12\n","\t\tTotal committed heap usage (bytes)=337641472\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-04-27 08:42:06,646 INFO streaming.StreamJob: Output directory: my_output\n"]}],"source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -D mapreduce.job.reduces=0 \\\n","    -input hello.txt \\\n","    -output my_output"]},{"cell_type":"markdown","metadata":{"id":"QZIE9yXOyyHJ"},"source":["## Verify the result"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1661,"status":"ok","timestamp":1714207345435,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"7Dt3tUI0yu5e","outputId":"a91220ca-e339-4ae7-e1f7-be5f0d64217d"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\tHello, World!\n"]}],"source":["!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"]},{"cell_type":"markdown","metadata":{"id":"hUGEUv99y3cM"},"source":["## Why a map-only application?\n","\n","The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n","\n","On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"]},{"cell_type":"markdown","metadata":{"id":"FhVVFEdKzGcI"},"source":["# Improved version of the MapReduce \"Hello, World!\" application\n","\n","Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6019,"status":"ok","timestamp":1714207374386,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"jLgMXX2jy0vC","outputId":"cf93b288-8417-4ef9-f914-c91934bc1ea4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Deleted my_output\n"]},{"name":"stderr","output_type":"stream","text":["2024-04-27 08:42:49,495 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","2024-04-27 08:42:51,469 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-27 08:42:51,719 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-27 08:42:51,719 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-27 08:42:51,750 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-27 08:42:52,153 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-27 08:42:52,205 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-27 08:42:52,450 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1396606840_0001\n","2024-04-27 08:42:52,450 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-27 08:42:52,619 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-27 08:42:52,620 INFO mapreduce.Job: Running job: job_local1396606840_0001\n","2024-04-27 08:42:52,636 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-27 08:42:52,638 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-27 08:42:52,643 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 08:42:52,643 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 08:42:52,689 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-27 08:42:52,696 INFO mapred.LocalJobRunner: Starting task: attempt_local1396606840_0001_m_000000_0\n","2024-04-27 08:42:52,726 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 08:42:52,728 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 08:42:52,755 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-27 08:42:52,763 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-04-27 08:42:52,780 INFO mapred.MapTask: numReduceTasks: 0\n","2024-04-27 08:42:52,795 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n","2024-04-27 08:42:52,803 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-04-27 08:42:52,806 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-04-27 08:42:52,806 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-04-27 08:42:52,807 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-04-27 08:42:52,807 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-04-27 08:42:52,807 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-04-27 08:42:52,808 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-04-27 08:42:52,809 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-04-27 08:42:52,809 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-04-27 08:42:52,809 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-04-27 08:42:52,809 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-04-27 08:42:52,810 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-04-27 08:42:52,843 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-27 08:42:52,845 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-27 08:42:52,845 INFO streaming.PipeMapRed: Records R/W=1/1\n","2024-04-27 08:42:52,845 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-27 08:42:52,848 INFO mapred.LocalJobRunner: \n","2024-04-27 08:42:52,858 INFO mapred.Task: Task:attempt_local1396606840_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-27 08:42:52,861 INFO mapred.LocalJobRunner: \n","2024-04-27 08:42:52,861 INFO mapred.Task: Task attempt_local1396606840_0001_m_000000_0 is allowed to commit now\n","2024-04-27 08:42:52,866 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1396606840_0001_m_000000_0' to file:/content/my_output\n","2024-04-27 08:42:52,867 INFO mapred.LocalJobRunner: Records R/W=1/1\n","2024-04-27 08:42:52,867 INFO mapred.Task: Task 'attempt_local1396606840_0001_m_000000_0' done.\n","2024-04-27 08:42:52,873 INFO mapred.Task: Final Counters for attempt_local1396606840_0001_m_000000_0: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=858459\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=15\n","\t\tTotal committed heap usage (bytes)=314572800\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-04-27 08:42:52,873 INFO mapred.LocalJobRunner: Finishing task: attempt_local1396606840_0001_m_000000_0\n","2024-04-27 08:42:52,874 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-27 08:42:53,630 INFO mapreduce.Job: Job job_local1396606840_0001 running in uber mode : false\n","2024-04-27 08:42:53,632 INFO mapreduce.Job:  map 100% reduce 0%\n","2024-04-27 08:42:53,634 INFO mapreduce.Job: Job job_local1396606840_0001 completed successfully\n","2024-04-27 08:42:53,640 INFO mapreduce.Job: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=858459\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=15\n","\t\tTotal committed heap usage (bytes)=314572800\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-04-27 08:42:53,640 INFO streaming.StreamJob: Output directory: my_output\n"]}],"source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -D mapreduce.job.reduces=0 \\\n","    -input hello.txt \\\n","    -output my_output \\\n","    -mapper '/bin/cat'"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2306,"status":"ok","timestamp":1714207384584,"user":{"displayName":"long vu","userId":"09832745920720751712"},"user_tz":-420},"id":"Sa1UDPr6zKKw","outputId":"bcb3fb59-d8d8-4c82-ec0a-80225d661ad3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello, World!\t\n"]}],"source":["!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FybO_OFIuQkS"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/groda/big_data/blob/master/MapReduce_Primer_HelloWorld.ipynb","timestamp":1714187987919}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
